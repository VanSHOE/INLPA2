# -*- coding: utf-8 -*-
"""NLP-tut2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uhrBKBGlRO7Cd1pkWn18BarX2YnXGdWh

The task: Language modelling using a neural 4gram model. Basically the input to the neural model is past 4 words and what we try to predict is the 5th word in a sequence.  

The dataset used is wikitext, can be downloaded from [this link](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)

# New Section
"""

# imports
import torch
from torch import nn, tensor
from torch.utils.data import Dataset, DataLoader
from torchtext.vocab import build_vocab_from_iterator
from tqdm import tqdm
import torch.optim as optim
from torch.nn import functional as F

"""# Tensors"""

x = torch.tensor([[1, 2, 3], [2, 5, 6]])
print("shape = ", (x.shape))
# it goes in the way 2 elements 3 elements each.
# x = torch.randn(4, 4)
# print(x.size())

y = x.view(1, -1)
y.size()
print("the elements are", y)

z = y.squeeze(0)
print(z.shape)

"""# Dataset

implementing a custom dataset
 - init: run at initialization
 - len: returns the num of samples in the dataset
 - getitem: given an index, returns the corresponding data sample
"""


class FourGramDataset(Dataset):
    def __init__(self, split: str):
        self.filename = 'wiki.' + split + '.tokens'
        lines = 0
        with open('wiki.' + split + '.tokens', 'r', encoding='utf8') as f:
            for line in f:
                lines += 1

        tokens = []
        with open(self.filename, 'r', encoding='utf8') as f:
            for line in tqdm(f, total=lines, desc="Building vocab"):
                line = line.strip()
                if len(line) == 0 or line[0] == '=': continue
                tokens += [[w] for w in line.split(' ')]
        self.vocab = build_vocab_from_iterator(tokens, min_freq=3,
                                               specials=['<PAD>', '<BOS>', '<EOS>'],
                                               special_first=False)
        # not adding <unk> because WikiText includes it

        self.vocab.set_default_index(self.vocab.get_stoi()['<unk>'])

        four_grams = []
        next_word = []
        with open(self.filename, 'r', encoding='utf8') as f:
            for line in tqdm(f, total=lines, desc="Generating data"):
                if len(line) < 2 or line[0] == '=': continue
                words = line.strip().split(' ')
                words = ['<BOS>'] * 4 + words + ['<EOS>']
                indices = [self.vocab[word] for word in words]
                for i in range(len(words) - 4):
                    four_grams.append(indices[i:i + 4])
                    next_word.append(indices[i + 4])

        self.four_grams = tensor(four_grams)
        self.next_word = tensor(next_word)

    def __len__(self) -> int:
        return self.four_grams.shape[0]

    def __getitem__(self, index: int):  # -> tuple[tensor, tensor]
        return self.four_grams[index], self.next_word[index]  # source, target


"""## Dataloader

- used to load data from the dataset in batches
"""

train_dataset = FourGramDataset('train')
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_dataset = FourGramDataset('valid')
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)

features, labels = next(iter(train_dataloader))
print(f"Feature batch shape: {features.size()}")
print(f"Labels batch shape: {labels.size()}")

"""# The model
- init: The architecture
- Forward: computing the output given an input. y = Model(x)
"""

# To use GPU 
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")


class FourGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(FourGramModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(4 * embedding_dim, 200)
        self.activ = nn.ReLU()
        self.linear2 = nn.Linear(200, vocab_size)
        self.dropout = nn.Dropout(0.25)

    def forward(self, inputs):
        # [bz, 4]
        # print(inputs.size())
        embeds = self.embeddings(inputs).flatten(1, 2)
        # [bz, 4*emb]
        # print(embeds.size())
        x = self.activ(self.linear1(embeds))
        # [bz, 200]
        logits = self.linear2(x)
        return logits


x = torch.randn(5, 2, 3)
print(x)
x.flatten(1, 2).size()

print(x.flatten(1, 2))

"""# The training loop 
- Set hyperparameters
- Pick loss function and optimizer
- Make train, validation/test loops

"""

# Hyperparameters
vocab_size = len(train_dataset.vocab)
print(vocab_size)
embedding_dim = 10
model = FourGramModel(vocab_size, embedding_dim)
epochs = 3
learning_rate = 0.01
# batch size is also a hyperparameter
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

print(vocab_size)

# The main loop 

print("Starting Training...")

for epoch in range(epochs):
    # Training loop
    train_loss = 0
    model.train()

    for batch in tqdm(train_dataloader, desc="Training"):
        x_train, y_train = batch
        optimizer.zero_grad()  # clears the gradients from prev iteration
        print('train', x_train.shape)
        output = model(x_train)  # forward pass
        print('test', output.shape, y_train.shape)
        loss = loss_fn(output, y_train)  # calculate loss

        # backprop
        loss.backward()  # compute gradients
        optimizer.step()  # update parameters

        train_loss += loss.item()

    # Validation loop
    val_loss = 0
    model.eval()
    for i, (x_val, y_val) in enumerate(val_dataloader):
        output = model(x_val)
        loss = loss_fn(output, y_val)
        val_loss += loss.item()

    print(
        f'Epoch {i + 1} \t\t Training Loss: {train_loss / len(train_dataloader)} \t\t Validation Loss: {val_loss / len(val_dataloader)}')

output

"""Some Additional steps: 
- Test loop and evaluation
- Hyperparameter tuning
- Early stopping (avoid overfitting)
"""
